{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8R3TBdU/Jm0+T+1Q03ird",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsangrebecca/BloomTech/blob/main/S5M3O3_ExpressAndExplainTheIntuitionAndInterpretationOfRidgeRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ5yjJYxGsPy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview\n",
        "When we are training a model, simple is usually better. With fewer parameters to determine, the model is easier to interpret. Also, with too many parameters there is a chance the model will fit the training data well but won't generalize to new data. This objective will focus on a specific type of regularization called ridge regression.\n",
        "\n",
        "Regularization\n",
        "The concept of regularization is to reduce variance in a model with a \"shrinkage\" penalty. A more formal definition of regularization is to add information or bias to prevent overfitting. In regression, there are two common types of regularization: ridge regression and lasso. Ridge regression uses a penalty where the tuning parameter is multiplied by the squared sum of all coefficients (L2). Lasso regression is similar, but the penalty is a tuning parameter multiplied by the sum of the absolute values of all the coefficients (L1).\n",
        "\n",
        "Ridge Regression\n",
        "A regression model determines the parameters that minimize the residual sum of squares (RSS). The basis of ridge regression is to minimize the residual sum of squares (RSS) plus some penalty multiplied by the sum of the squares of the coefficients. The larger the penalty, the more the parameters are penalized, especially large parameters. The penalty or tuning parameter is called both lambda and alpha; lambda means something in Python already, so we'll use alpha for this discussion.\n",
        "\n",
        "Note: the term \"ridge\" refers to the shape of the function we are trying to minimize. We turn a ridge into a peak where the parameters that minimize the RSS plus the penalty term are located at this peak.\n",
        "\n",
        "Standardization\n",
        "Another consideration when using any type of regularization technique is standardization. If the variables are on different scales, the larger variable will have a different contribution to the penalized terms. For example, if we were predicting weight from height, if height is in meters, a change of one unit will be a large change in weight. If height is in a smaller unit like centimeters, a unit change will be a much smaller change in weight.\n",
        "\n",
        "Follow Along\n",
        "Now that we have gone over the concepts of regularization and ridge regression, let's look at the math illustrated with a few plots to help us understand the concept better.\n",
        "\n",
        "Sum of Squares\n",
        "When we fit a linear regression, we want to find the parameters (slope and intercept) that minimize the sum of the squares of the distance from each data point to the best-fit line. In the plot below on the left, the sum of the errors is shown by the solid magenta line. Now let's take two of these data points and call them our \"training data\" (plot on the right). With two points, we can fit a perfect line which has no error. But the RSS error has increased, which is indicated by the gray \"model error\" line in the plot on the right.\n",
        "\n",
        "Example1\n",
        "\n",
        "With ridge regression, we want to introduce a small amount of bias in the training data. In other words, we don't want the line to fit these two training points perfectly. With this new line there is a small amount of error (red lines). With this new, slightly biased model, the RSS for the rest of the data has decreased.\n",
        "\n",
        "Example2"
      ],
      "metadata": {
        "id": "-x5nP6HpH9Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFI4rQ8TIA_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}