{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz2iYc5Qh+b1EMacwb3AWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsangrebecca/BloomTech/blob/main/Sprint7/Module3/O2_UseXGBoostForGradientBoosting_BootstrapAggregating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrap aggregating"
      ],
      "metadata": {
        "id": "_ntrAHXeSKws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap aggregating, commonly known as bagging, is an ensemble learning technique that aims to improve the stability and accuracy of machine learning models. It was introduced by Leo Breiman in 1996. Bagging is particularly useful for reducing variance and preventing overfitting, especially in high-variance models like decision trees.\n",
        "\n",
        "Here's how bagging works:\n",
        "\n",
        "**Bootstrap Sampling**: The process begins by creating multiple bootstrap samples from the original dataset. Bootstrap sampling involves randomly selecting samples with replacement from the original dataset. This means that some instances may be repeated in a bootstrap sample, while others may be left out.\n",
        "\n",
        "**Model Training**: A base model (classifier or regressor) is trained on each of these bootstrap samples independently. As a result, multiple models are created, each trained on a slightly different subset of the data.\n",
        "\n",
        "**Aggregation**: The predictions of individual models are then combined or averaged to obtain the final prediction. The aggregation helps to reduce variance and improve the overall performance of the model.\n",
        "\n",
        "The key idea behind bagging is that by training models on different subsets of the data and combining their predictions, the ensemble model becomes more robust and generalizes better to new, unseen data.\n",
        "\n",
        "Random Forest, a popular ensemble algorithm, is an extension of bagging and is primarily used with decision trees as base models. In Random Forest, not only are different subsets of data used, but also different subsets of features are considered for each tree, adding an extra layer of diversity to the ensemble."
      ],
      "metadata": {
        "id": "4WJ42SvIGWcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting"
      ],
      "metadata": {
        "id": "b8MyfSIwSHjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is another ensemble learning technique in machine learning, like bagging. However, unlike bagging, boosting focuses on combining weak learners (models that perform slightly better than random chance) to create a strong learner. The primary goal of boosting is to sequentially improve the performance of a model by giving more weight to misclassified instances in the training process.\n",
        "\n",
        "Here's how boosting typically works:\n",
        "\n",
        "**Base Model Training**: The first base model is trained on the original dataset.\n",
        "\n",
        "**Instance Weighting**: Instances that are misclassified by the first model are given higher weights, and those that are classified correctly receive lower weights. This allows the subsequent models to pay more attention to the previously misclassified instances.\n",
        "\n",
        "**Model Iteration**: Additional models are trained, and at each iteration, the algorithm focuses more on the instances that were misclassified in the previous iterations.\n",
        "\n",
        "**Weighted Aggregation**: The predictions of all models are combined, and the final prediction is made based on a weighted sum of the individual model predictions. Models that perform well on difficult instances are given higher weights in the final ensemble.\n",
        "\n",
        "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). These algorithms differ in their specific strategies for assigning weights to instances and combining the predictions of base models.\n",
        "\n",
        "Boosting tends to perform well in practice and is particularly effective when applied to weak models. It often helps improve accuracy and generalization on complex tasks. However, boosting is more prone to overfitting compared to bagging, and care must be taken to tune hyperparameters to prevent overfitting.\n",
        "\n",
        "In summary, boosting is an ensemble learning technique that focuses on improving the performance of weak learners by assigning higher weights to misclassified instances and sequentially training models to correct errors made by previous models."
      ],
      "metadata": {
        "id": "RIjf_Fb4SP9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ],
      "metadata": {
        "id": "ASv4UD03SbiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting is a powerful machine learning technique that belongs to the family of boosting algorithms. It builds a strong predictive model by sequentially adding weak learners (typically decision trees) to the ensemble, with each new learner correcting the errors of the previous ones. The key idea behind gradient boosting is to minimize a loss function, which measures the difference between the predicted and actual values.\n",
        "\n",
        "Here is a simplified overview of the gradient boosting process:\n",
        "\n",
        "**Base Model**: The first weak learner is trained on the original dataset.\n",
        "\n",
        "**Residual Calculation**: The difference between the actual values and the predicted values of the first model (residuals) is computed. The subsequent weak learners are then trained to predict these residuals.\n",
        "\n",
        "**Learning Rate**: A small learning rate is introduced to control the contribution of each weak learner. This helps in preventing overfitting and makes the learning process more robust.\n",
        "\n",
        "**Sequential Training**: The next weak learner is trained to correct the errors made by the combined predictions of the previous models. The process is repeated iteratively.\n",
        "\n",
        "**Final Prediction**: The final prediction is the sum of the predictions from all the weak learners, each scaled by the learning rate.\n",
        "\n",
        "The \"gradient\" in gradient boosting refers to the optimization of the loss function using gradient descent. The algorithm minimizes the loss by iteratively moving in the direction of steepest decrease in the loss function. This ensures that each new weak learner focuses on the instances that were poorly predicted by the previous models.\n",
        "\n",
        "Gradient boosting can be applied to both regression and classification problems. It is known for its high predictive accuracy and robustness. Popular implementations of gradient boosting include XGBoost (eXtreme Gradient Boosting), LightGBM (Light Gradient Boosting Machine), and scikit-learn's GradientBoostingRegressor and GradientBoostingClassifier.\n",
        "\n",
        "In summary, gradient boosting is an ensemble learning technique that builds a strong predictive model by sequentially adding weak learners and optimizing a loss function through gradient descent. It has proven to be effective in a wide range of machine learning tasks."
      ],
      "metadata": {
        "id": "XU8jDHWoUka1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create X, y and training/test sets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Import the classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.5, random_state=42)\n",
        "ada_classifier.fit(X_train,y_train)\n",
        "\n",
        "print('Validation Accuracy: Adaboost', ada_classifier.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3alRLLJUvK0",
        "outputId": "9efffbfe-7c2e-45e7-e505-d62b2f8a7ce4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: Adaboost 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load xgboost and fit the model\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xg_classifier = XGBClassifier(n_estimators=50, random_state=42, eval_metric='merror')\n",
        "\n",
        "xg_classifier.fit(X_train,y_train)\n",
        "\n",
        "print('Validation Accuracy: Adaboost', xg_classifier.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvrovVh0VF5V",
        "outputId": "ad527274-6466-4140-f2a7-24f61509a058"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: Adaboost 0.9833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We increased the accuracy a little bit here, but in order to make a decision on which type of classifier is better, we would need more data. **The xgboost method is a popular machine learning algorithm and is behind many winning entries in Kaggle competitions**, so it's not a bad choice to begin with."
      ],
      "metadata": {
        "id": "eJPQyeBPVNMr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLNTC4U2VQUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}